# LLM Upgrade Recommendations - Quick Reference
**Date**: February 6, 2026

## Executive Summary

Based on comprehensive research of small LLMs released through early 2026, **qwen3:4b** is recommended as the primary upgrade path for the payphone project, offering superior quality while maintaining good speed on Pi 5.

---

## Top 5 Models for Pi 5 (16GB)

| Rank | Model | Size | Est. TPS | RAM | Best For |
|------|-------|------|----------|-----|----------|
| ðŸ¥‡ | **qwen3:4b** | 4B | 4-5 | 3-4GB | **Best overall** - quality + speed balance |
| ðŸ¥ˆ | **llama3.2:3b** | 3B | 4.6 | 4-6GB | **Proven** - current baseline, reliable |
| ðŸ¥‰ | **phi4-mini** | 3.8B | 3-4 | 3-4GB | **Most concise** - efficiency champion |
| 4 | **gemma3:1b** | 1B | 6-8 | 1-2GB | **Fastest** - speed-critical tasks |
| 5 | **ministral-3:3b** | 3B | 3-4 | 3-4GB | **Token-efficient** - ultra-concise responses |

---

## Quick Start Commands

```bash
# Download top recommendations
ollama pull qwen3:4b              # Primary recommendation
ollama pull llama3.2:3b           # Current baseline
ollama pull phi4-mini             # For concise responses
ollama pull gemma3:1b             # For maximum speed

# Test performance
ollama run qwen3:4b "Tell me a joke"
ollama run phi4-mini "Explain quantum computing in one sentence"
```

---

## Model Comparison Matrix

### Qwen3:4b (NEW - Top Recommendation)
```
Parameters:     4B
Context:        128K tokens
Release:        April 2025
Ollama:         qwen3:4b
License:        Apache 2.0

Performance (Q4_K_M):
  Speed:        4-5 TPS (estimated)
  RAM:          3-4GB
  Model Size:   ~2.6GB

Strengths:
  âœ… 50% density improvement over Qwen2.5
  âœ… Best multilingual support (100+ languages)
  âœ… Excellent instruction following
  âœ… Natural conversational quality
  âœ… Toggle-able thinking mode
  âœ… Apache 2.0 license (commercial use OK)

Weaknesses:
  âš ï¸ Not benchmarked on Pi 5 yet (needs testing)
  âš ï¸ Slightly slower than 3B models

Use Cases:
  â€¢ Primary conversational AI
  â€¢ Multilingual support
  â€¢ Complex instructions
  â€¢ Best quality responses

Recommendation: PRIMARY UPGRADE
```

### Llama3.2:3b (Current Default)
```
Parameters:     3B
Context:        128K tokens
Release:        October 2024
Ollama:         llama3.2:3b
License:        Llama Community

Performance (Q4_K_M):
  Speed:        4.59 TPS (measured)
  RAM:          4-6GB
  Model Size:   ~2.0GB

Strengths:
  âœ… Proven performance on Pi 5
  âœ… Reliable conversational quality
  âœ… Strong community support
  âœ… Good instruction following
  âœ… Well-tested stability

Weaknesses:
  âš ï¸ Higher RAM usage than newer models
  âš ï¸ Superseded by newer alternatives

Use Cases:
  â€¢ Fallback/baseline reference
  â€¢ Production stability
  â€¢ Known-good configuration

Recommendation: KEEP AS FALLBACK
```

### Phi4-mini (NEW - Efficiency Champion)
```
Parameters:     3.8B
Context:        128K tokens
Release:        February 2025
Ollama:         phi4-mini
License:        MIT

Performance (Q4_K_M):
  Speed:        3-4 TPS (estimated)
  RAM:          3-4GB
  Model Size:   ~2.5GB

Strengths:
  âœ… Best efficiency in class
  âœ… Most concise responses
  âœ… Multimodal (text/image/audio)
  âœ… Excellent instruction following
  âœ… Function calling support
  âœ… Strong reasoning/math
  âœ… MIT license

Weaknesses:
  âš ï¸ Slightly slower than 3B models
  âš ï¸ Not benchmarked on Pi 5 yet

Use Cases:
  â€¢ Ultra-concise responses
  â€¢ Token-budget optimization
  â€¢ Complex instructions
  â€¢ Math/reasoning tasks

Recommendation: TEST FOR CONCISENESS
```

### Gemma3:1b (Speed Champion)
```
Parameters:     1B
Context:        128K tokens
Release:        August 2025
Ollama:         gemma3:1b
License:        Gemma Terms of Use

Performance (Q4_K_M):
  Speed:        6-8 TPS (estimated)
  RAM:          1-2GB
  Model Size:   ~700MB

Strengths:
  âœ… Fastest generation speed
  âœ… Minimal memory footprint
  âœ… 140+ languages
  âœ… Good instruction following

Weaknesses:
  âš ï¸ Lower quality than larger models
  âš ï¸ Limited reasoning capability
  âš ï¸ 1B size constraints

Use Cases:
  â€¢ Speed-critical features
  â€¢ Wake word responses
  â€¢ Simple queries
  â€¢ Resource-constrained scenarios

Recommendation: USE FOR SPEED-CRITICAL TASKS
```

---

## Testing Plan

### Phase 1: Baseline Validation
```bash
# Verify current performance
ollama pull llama3.2:3b
ollama run llama3.2:3b

# Benchmark queries:
# 1. "Tell me a joke"
# 2. "What time is it in Tokyo?"
# 3. "Explain quantum computing in one sentence"

# Measure:
# - Tokens per second
# - Response quality
# - Response length (token count)
# - Time to first token
```

### Phase 2: Quality Upgrade Test
```bash
# Test primary recommendation
ollama pull qwen3:4b
ollama run qwen3:4b

# Compare against llama3.2:3b using same queries
# Metrics:
# - TPS (should be ~4-5)
# - RAM usage (should be ~3-4GB)
# - Conversational quality
# - Multilingual capability
```

### Phase 3: Efficiency Test
```bash
# Test conciseness champion
ollama pull phi4-mini
ollama run phi4-mini

# Compare response lengths
# Metrics:
# - Average tokens per response
# - Response quality vs length
# - Instruction following accuracy
```

### Phase 4: Speed Test
```bash
# Test fastest option
ollama pull gemma3:1b
ollama run gemma3:1b

# Measure maximum speed
# Metrics:
# - TPS (should be 6-8+)
# - Quality trade-offs
# - Suitability for simple queries
```

---

## Integration Changes Required

### Minimal Changes (Configuration Only)

Update `payphone-app/config/settings.py`:

```python
class Settings(BaseSettings):
    # LLM Configuration
    OLLAMA_BASE_URL: str = "http://10.10.10.11:11434"
    OLLAMA_MODEL: str = "qwen3:4b"  # Changed from llama3.2:3b
    OLLAMA_TEMPERATURE: float = 0.7
    OLLAMA_TIMEOUT: float = 30.0
```

### Advanced Changes (Dynamic Model Selection)

Add model selection logic:

```python
# config/settings.py
class Settings(BaseSettings):
    OLLAMA_MODEL_DEFAULT: str = "qwen3:4b"
    OLLAMA_MODEL_FAST: str = "gemma3:1b"
    OLLAMA_MODEL_QUALITY: str = "phi4-mini"

# core/pipeline.py
def select_model(query_complexity: str) -> str:
    """Select model based on query complexity"""
    if query_complexity == "simple":
        return settings.OLLAMA_MODEL_FAST  # gemma3:1b (6-8 TPS)
    elif query_complexity == "complex":
        return settings.OLLAMA_MODEL_QUALITY  # phi4-mini
    else:
        return settings.OLLAMA_MODEL_DEFAULT  # qwen3:4b
```

---

## Performance Optimization

### Pi 5 Configuration

```bash
# /etc/environment or ~/.bashrc
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_KEEP_ALIVE=5m

# Verify active cooling
vcgencmd measure_temp
# Target: < 70Â°C under load

# Check available RAM
free -h
# Should have 12GB+ free for 4B models
```

### Model Quantization

Use Q4_K_M quantization (Ollama default for most models):
- Best balance of speed/quality/RAM
- ~0.5-0.7 bytes per parameter
- Example: 4B model = ~2.6GB file, 3-4GB RAM usage

Avoid:
- Q8 or unquantized (too slow, excessive RAM)
- Q2/Q3 (quality degradation)

---

## Expected Improvements

### Switching from llama3.2:3b to qwen3:4b

| Metric | Current (llama3.2:3b) | Expected (qwen3:4b) | Change |
|--------|----------------------|---------------------|--------|
| Speed (TPS) | 4.59 | 4-5 | ~Same |
| RAM Usage | 4-6GB | 3-4GB | -25% to -40% |
| Quality | Good | Excellent | +15% |
| Multilingual | Good | Best | +50% |
| Model Size | 2.0GB | 2.6GB | +30% |

### Switching to phi4-mini (for conciseness)

| Metric | Current (llama3.2:3b) | Expected (phi4-mini) | Change |
|--------|----------------------|---------------------|--------|
| Speed (TPS) | 4.59 | 3-4 | -15% to -25% |
| Response Length | 100 tokens | 60-80 tokens | -20% to -40% |
| Efficiency | Good | Excellent | +30% |
| Quality | Good | Excellent | +10% |

### Using gemma3:1b (for speed)

| Metric | Current (llama3.2:3b) | Expected (gemma3:1b) | Change |
|--------|----------------------|---------------------|--------|
| Speed (TPS) | 4.59 | 6-8 | +30% to +75% |
| RAM Usage | 4-6GB | 1-2GB | -60% to -75% |
| Quality | Good | Fair | -20% |
| Latency | ~1s | ~0.5s | -50% |

---

## Decision Matrix

### Use qwen3:4b if:
- âœ… Quality is priority
- âœ… Multilingual support needed
- âœ… 4-5 TPS is acceptable
- âœ… Have 4GB+ free RAM

### Use phi4-mini if:
- âœ… Concise responses critical
- âœ… Token budget matters
- âœ… 3-4 TPS is acceptable
- âœ… Need multimodal support

### Use gemma3:1b if:
- âœ… Speed is absolute priority
- âœ… Simple queries only
- âœ… Can accept quality trade-offs
- âœ… RAM is constrained

### Keep llama3.2:3b if:
- âœ… Stability is paramount
- âœ… Proven performance required
- âœ… No time for testing
- âœ… Current quality is sufficient

---

## Risk Assessment

### Low Risk (Recommended)
- âœ… Testing qwen3:4b alongside llama3.2:3b
- âœ… A/B testing different models
- âœ… Gradual rollout with fallback

### Medium Risk
- âš ï¸ Immediate switch to untested model
- âš ï¸ No fallback configuration
- âš ï¸ Production deployment without validation

### High Risk (Avoid)
- âŒ Using 8B+ models (too slow)
- âŒ Using reasoning models (latency issues)
- âŒ Disabling quantization (RAM overflow)
- âŒ Running multiple models (RAM exhaustion)

---

## Monitoring Metrics

Track these metrics during testing:

```python
# Metrics to log
{
    "model": "qwen3:4b",
    "quantization": "Q4_K_M",
    "query": "Tell me a joke",
    "response_time_ms": 1250,
    "tokens_generated": 45,
    "tokens_per_second": 3.6,
    "ram_usage_mb": 3400,
    "cpu_temp_c": 68,
    "quality_rating": 4.5  # Manual rating
}
```

### Success Criteria

qwen3:4b passes if:
- âœ… TPS â‰¥ 4.0 (within 10% of llama3.2:3b)
- âœ… RAM usage â‰¤ 4GB
- âœ… Response quality â‰¥ llama3.2:3b
- âœ… No stability issues over 1 hour
- âœ… CPU temp < 75Â°C sustained

---

## Next Steps

### Immediate (This Week)
1. âœ… Download qwen3:4b: `ollama pull qwen3:4b`
2. âœ… Run benchmark tests
3. âœ… Compare against llama3.2:3b
4. âœ… Document results

### Short Term (Next Sprint)
1. Update configuration to qwen3:4b if tests pass
2. Implement model selection logic
3. Add gemma3:1b for speed-critical features
4. Update documentation

### Long Term (Next Quarter)
1. Implement dynamic model switching
2. Add performance monitoring
3. Fine-tune for payphone use case
4. Explore specialized models (if needed)

---

## Quick Reference Table

| Model | Speed | Quality | RAM | Use For |
|-------|-------|---------|-----|---------|
| qwen3:4b | â­â­â­â­ | â­â­â­â­â­ | 3-4GB | **Primary AI** |
| llama3.2:3b | â­â­â­â­â­ | â­â­â­â­ | 4-6GB | **Fallback** |
| phi4-mini | â­â­â­â­ | â­â­â­â­â­ | 3-4GB | **Concise** |
| gemma3:1b | â­â­â­â­â­ | â­â­â­ | 1-2GB | **Fast** |
| ministral-3:3b | â­â­â­â­ | â­â­â­â­ | 3-4GB | **Efficient** |

---

## Conclusion

**Primary Recommendation**: Upgrade to **qwen3:4b** as the main LLM for the payphone project. It offers the best balance of quality, speed, and resource efficiency for conversational AI on Raspberry Pi 5.

**Fallback Strategy**: Keep llama3.2:3b as a proven alternative if issues arise.

**Speed Optimization**: Add gemma3:1b for time-critical features like wake word responses or simple queries.

**Testing Required**: All recommendations are based on research and estimates. Direct benchmarking on Pi 5 hardware is essential before production deployment.

---

**Report Generated**: February 6, 2026
**Full Research**: See `small-llm-research-2026.md` for comprehensive analysis
